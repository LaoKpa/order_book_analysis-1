{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data uploading and feature ingeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loads necessary packages\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from finta import TA\n",
    "from utils.append_indicators import append_indicators\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates MySQL connection object\n",
    "'''\n",
    "\n",
    "engine = create_engine(\n",
    "    'mysql://Quotermain:Quotermain233@192.168.0.105:3306/trading_data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates collections with timeframes \n",
    "for candles and indicators\n",
    "'''\n",
    "\n",
    "dict_of_tf = {\n",
    "    '1_': 480, #problem\n",
    "    '4_': 120,\n",
    "    '15_': 32,\n",
    "    '30_': 16, #problem\n",
    "    '2_': 240, #problem\n",
    "    '120_': 4,\n",
    "    '20_': 24, #problem\n",
    "    '240_': 2,\n",
    "    '5_': 96,\n",
    "    '6_': 80,\n",
    "    '10_': 48, #problem\n",
    "    '3_': 160,\n",
    "    '60_': 8\n",
    "}\n",
    "\n",
    "list_with_indicators = [\n",
    "    'SMA', 'SMM', 'EMA_13', 'EMA_26', 'EMA_DIF', 'DEMA', 'TEMA', 'TRIMA', 'TRIX',\n",
    "    'VAMA', 'ER', 'ZLEMA', 'WMA', 'HMA', 'EVWMA', 'VWAP', 'SMMA', 'MOM',\n",
    "    'ROC', 'RSI', 'IFT_RSI', 'TR', 'ATR', 'BBWIDTH', 'PERCENT_B', 'ADX', 'STOCH', \n",
    "    'STOCHD', 'STOCHRSI', 'WILLIAMS', 'UO', 'AO', 'TP', 'ADL', 'CHAIKIN', 'MFI',\n",
    "    'OBV', 'WOBV', 'VZO', 'EFI', 'CFI', 'EMV', 'CCI', 'COPP', 'CMO', 'FISH', \n",
    "    'SQZMI', 'VPT', 'FVE', 'VFI', 'MSD', 'return'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the LIMITED data for SBER,\n",
    "sets the datetime index, drops\n",
    "duplicates and nulls\n",
    "'''\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM SBER_train LIMIT 100000', engine)\n",
    "df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "df = df.set_index('date_time')\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Appends columns with target variable\n",
    "as max distance to low and high during\n",
    "time_range\n",
    "'''\n",
    "\n",
    "df['dist_to_max_per_range'] = np.array(df[['close']]\\\n",
    "    .iloc[::-1].rolling(30, min_periods=1).max().iloc[::-1])\\\n",
    "    - np.array(df[['close']])\n",
    "\n",
    "df['dist_to_min_per_range'] = np.array(df[['close']])\\\n",
    "    - np.array(df[['close']]\\\n",
    "    .iloc[::-1].rolling(30, min_periods=1).min().iloc[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates proportion of each row \n",
    "in order book to the apropriate \n",
    "section(bid or offer)\n",
    "'''\n",
    "\n",
    "df_offer_count_proportion = df.loc[:, 'offer_count_10':'offer_count_1']\\\n",
    "    .div(df.loc[:, 'offer_count_10':'offer_count_1'].sum(axis=1), axis=0)\n",
    "\n",
    "df_bid_count_proportion = df.loc[:, 'bid_count_10':'bid_count_1']\\\n",
    "    .div(df.loc[:, 'bid_count_10':'bid_count_1'].sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates offer/bid ratio per row\n",
    "and drops columns with separate bids\n",
    "and asks\n",
    "'''\n",
    "\n",
    "offer_bid_ratio = pd.DataFrame(df.loc[:, 'offer_count_10':'offer_count_1'].sum(axis=1) /\\\n",
    "    df.loc[:, 'bid_count_10':'bid_count_1'].sum(axis=1))\n",
    "\n",
    "df = df.drop([\n",
    "    'offer_count_10', 'offer_count_9', 'offer_count_8', 'offer_count_7',\n",
    "    'offer_count_6', 'offer_count_5', 'offer_count_4', 'offer_count_3',\n",
    "    'offer_count_2', 'offer_count_1', 'bid_count_10', 'bid_count_9', \n",
    "    'bid_count_8', 'bid_count_7',\n",
    "    'bid_count_6', 'bid_count_5', 'bid_count_4', 'bid_count_3',\n",
    "    'bid_count_2', 'bid_count_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concatenates single df for analysis\n",
    "and drops nulls\n",
    "'''\n",
    "\n",
    "list_of_dfs = [\n",
    "    df,\n",
    "    df_offer_count_proportion, \n",
    "    df_bid_count_proportion, \n",
    "    offer_bid_ratio\n",
    "]\n",
    "\n",
    "temp_df = pd.concat(list_of_dfs, axis=1)\n",
    "\n",
    "temp_df = temp_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quotermin/anaconda3/lib/python3.7/site-packages/finta/finta.py:1460: RuntimeWarning: divide by zero encountered in log\n",
      "  (log((1 + _smooth) / (1 - _smooth))).ewm(span=3).mean(),\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Appends indicators and drops nulls\n",
    "'''\n",
    "\n",
    "for key in dict_of_tf:\n",
    "    temp_df = append_indicators(\n",
    "        temp_df, key, list_with_indicators\n",
    "    )\n",
    "\n",
    "temp_df = temp_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Copies the df with uploaded indicators\n",
    "to avoid waiting\n",
    "'''\n",
    "\n",
    "df_to_analyze = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates column to indicate movement above and below\n",
    "median movement of the price as the target variable\n",
    "'''\n",
    "\n",
    "conditions = [\n",
    "    np.logical_and(\n",
    "        df_to_analyze['dist_to_max_per_range'] > np.percentile(\n",
    "            df_to_analyze['dist_to_max_per_range'], 50\n",
    "        ),\n",
    "        df_to_analyze['dist_to_min_per_range'] < np.percentile(\n",
    "            df_to_analyze['dist_to_min_per_range'], 50\n",
    "        )\n",
    "    ),\n",
    "    np.logical_and(\n",
    "        df_to_analyze['dist_to_max_per_range'] < np.percentile(\n",
    "            df_to_analyze['dist_to_max_per_range'], 50\n",
    "        ),\n",
    "        df_to_analyze['dist_to_min_per_range'] > np.percentile(\n",
    "            df_to_analyze['dist_to_min_per_range'], 50\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "choices = [1, 2]\n",
    "df_to_analyze['y'] = np.select(conditions, choices, default=0)\n",
    "df_to_analyze.y=df_to_analyze.y.shift(-1)\n",
    "df_to_analyze = df_to_analyze.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Splits the data into features and targets\n",
    "and further splits it into train and test\n",
    "'''\n",
    "\n",
    "X = df_to_analyze.drop(['dist_to_max_per_range', 'dist_to_min_per_range', 'y'], axis=1)\n",
    "y = df_to_analyze.y\n",
    "\n",
    "#Creates the oldest data as the train set and the newest as the test set\n",
    "train_size = int(df_to_analyze.shape[0] * 0.75)\n",
    "X_train = X.iloc[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X.iloc[train_size:, :]\n",
    "y_test = y.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts the data to the DMatrix\n",
    "'''\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates the model, fits it,\n",
    "makes predictions\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'max_depth': 15,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'n_gpus': 0\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, dtrain)\n",
    "\n",
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER\n",
      "Clf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.22      0.23      1832\n",
      "         1.0       0.39      0.62      0.48      3045\n",
      "         2.0       0.43      0.22      0.29      3309\n",
      "\n",
      "   micro avg       0.37      0.37      0.37      8186\n",
      "   macro avg       0.35      0.35      0.33      8186\n",
      "weighted avg       0.37      0.37      0.35      8186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prints classification report\n",
    "for both models\n",
    "'''\n",
    "\n",
    "print('SBER')\n",
    "print('Clf')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting features and modeling again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Selects features according importance\n",
    "'''\n",
    "\n",
    "# Builds a dictionary with sorted features\n",
    "dict_with_features = bst.get_score(importance_type='weight')\n",
    "dict_with_features_sorted = {\n",
    "    k: v for k, v in \n",
    "    sorted(dict_with_features.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "\n",
    "# Creates a list with sorted feature names\n",
    "selected_features = list(dict_with_features_sorted.keys())[:10]\n",
    "\n",
    "# Selects features\n",
    "X_selected = df_to_analyze[selected_features]\n",
    "X_train_selected = X.iloc[:train_size, :]\n",
    "X_test_selected = X.iloc[train_size:, :]\n",
    "\n",
    "# Converts features to a DMatrix\n",
    "dtrain_selected = xgb.DMatrix(data=X_train_selected, label=y_train)\n",
    "dtest_selected = xgb.DMatrix(data=X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
