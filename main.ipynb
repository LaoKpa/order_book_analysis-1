{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data uploading and feature ingeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loads necessary packages\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from finta import TA\n",
    "from utils.append_indicators import append_indicators\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates MySQL connection object\n",
    "'''\n",
    "\n",
    "engine = create_engine(\n",
    "    'mysql://Quotermain:Quotermain233@192.168.0.105:3306/trading_data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates collections with timeframes \n",
    "for candles and indicators\n",
    "'''\n",
    "\n",
    "dict_of_tf = {\n",
    "    '1_': 480, #problem\n",
    "    '4_': 120,\n",
    "    '15_': 32,\n",
    "    '30_': 16, #problem\n",
    "    '2_': 240, #problem\n",
    "    '120_': 4,\n",
    "    '20_': 24, #problem\n",
    "    '240_': 2,\n",
    "    '5_': 96,\n",
    "    '6_': 80,\n",
    "    '10_': 48, #problem\n",
    "    '3_': 160,\n",
    "    '60_': 8\n",
    "}\n",
    "\n",
    "list_with_indicators = [\n",
    "    'SMA', 'SMM', 'EMA_13', 'EMA_26', 'EMA_DIF', 'DEMA', 'TEMA', 'TRIMA', 'TRIX',\n",
    "    'VAMA', 'ER', 'ZLEMA', 'WMA', 'HMA', 'EVWMA', 'VWAP', 'SMMA', 'MOM',\n",
    "    'ROC', 'RSI', 'IFT_RSI', 'TR', 'ATR', 'BBWIDTH', 'PERCENT_B', 'ADX', 'STOCH', \n",
    "    'STOCHD', 'STOCHRSI', 'WILLIAMS', 'UO', 'AO', 'TP', 'ADL', 'CHAIKIN', 'MFI',\n",
    "    'OBV', 'WOBV', 'VZO', 'EFI', 'CFI', 'EMV', 'CCI', 'COPP', 'CMO', 'FISH', \n",
    "    'SQZMI', 'VPT', 'FVE', 'VFI', 'MSD', 'return'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the LIMITED data for SBER,\n",
    "sets the datetime index, drops\n",
    "duplicates and nulls\n",
    "'''\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM SBER_train LIMIT 300000', engine)\n",
    "df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "df = df.set_index('date_time')\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates proportion of each row \n",
    "in order book to the apropriate \n",
    "section(bid or offer)\n",
    "'''\n",
    "\n",
    "df_offer_count_proportion = df.loc[:, 'offer_count_10':'offer_count_1']\\\n",
    "    .div(df.loc[:, 'offer_count_10':'offer_count_1'].sum(axis=1), axis=0)\n",
    "\n",
    "df_bid_count_proportion = df.loc[:, 'bid_count_10':'bid_count_1']\\\n",
    "    .div(df.loc[:, 'bid_count_10':'bid_count_1'].sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates offer/bid ratio per row\n",
    "and drops columns with separate bids\n",
    "and asks\n",
    "'''\n",
    "\n",
    "offer_bid_ratio = pd.DataFrame(df.loc[:, 'offer_count_10':'offer_count_1'].sum(axis=1) /\\\n",
    "    df.loc[:, 'bid_count_10':'bid_count_1'].sum(axis=1))\n",
    "\n",
    "df = df.drop([\n",
    "    'offer_count_10', 'offer_count_9', 'offer_count_8', 'offer_count_7',\n",
    "    'offer_count_6', 'offer_count_5', 'offer_count_4', 'offer_count_3',\n",
    "    'offer_count_2', 'offer_count_1', 'bid_count_10', 'bid_count_9', \n",
    "    'bid_count_8', 'bid_count_7',\n",
    "    'bid_count_6', 'bid_count_5', 'bid_count_4', 'bid_count_3',\n",
    "    'bid_count_2', 'bid_count_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concatenates single df for analysis\n",
    "and drops nulls\n",
    "'''\n",
    "\n",
    "list_of_dfs = [\n",
    "    df,\n",
    "    df_offer_count_proportion, \n",
    "    df_bid_count_proportion, \n",
    "    offer_bid_ratio\n",
    "]\n",
    "\n",
    "temp_df = pd.concat(list_of_dfs, axis=1)\n",
    "\n",
    "temp_df = temp_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quotermin/anaconda3/lib/python3.7/site-packages/finta/finta.py:1460: RuntimeWarning: divide by zero encountered in log\n",
      "  (log((1 + _smooth) / (1 - _smooth))).ewm(span=3).mean(),\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-f67966a76e9f>\", line 7, in <module>\n",
      "    temp_df, key, list_with_indicators\n",
      "  File \"/home/quotermin/ml/trading/candles_ticks_orderbook/utils/append_indicators.py\", line 90, in append_indicators\n",
      "    df[tf + indicator] = eval('TA.' + indicator + '(ohlcv)')\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/finta/finta.py\", line 606, in IFT_RSI\n",
      "    for i in _chunks(rev, wma_period):\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/finta/finta.py\", line 594, in _chunks\n",
      "    c = rev.iloc[i[0] : i[0] + period]\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1478, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2080, in _getitem_axis\n",
      "    return self._get_slice_axis(key, axis=axis)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2050, in _get_slice_axis\n",
      "    return self._slice(slice_obj, axis=axis, kind='iloc')\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 150, in _slice\n",
      "    return self.obj._slice(obj, axis=axis, kind=kind)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 762, in _slice\n",
      "    return self._get_values(slobj)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 878, in _get_values\n",
      "    fastpath=True).__finalize__(self)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 281, in __init__\n",
      "    self.name = name\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\", line 4388, in __setattr__\n",
      "    object.__getattribute__(self, name)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 403, in name\n",
      "    return self._name\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\", line 4362, in __getattr__\n",
      "    def __getattr__(self, name):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/quotermin/anaconda3/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Appends indicators and drops nulls\n",
    "'''\n",
    "\n",
    "for key in dict_of_tf:\n",
    "    temp_df = append_indicators(\n",
    "        temp_df, key, list_with_indicators\n",
    "    )\n",
    "\n",
    "temp_df = temp_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Copies the df with uploaded indicators\n",
    "to avoid waiting\n",
    "'''\n",
    "\n",
    "df_to_analyze = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Appends columns with target variable\n",
    "as max distance to low and high during\n",
    "time_range\n",
    "'''\n",
    "\n",
    "df['dist_to_max_per_range'] = np.array(df[['close']]\\\n",
    "    .iloc[::-1].rolling(100, min_periods=1).max().iloc[::-1])\\\n",
    "    - np.array(df[['close']])\n",
    "\n",
    "df['dist_to_min_per_range'] = np.array(df[['close']])\\\n",
    "    - np.array(df[['close']]\\\n",
    "    .iloc[::-1].rolling(100, min_periods=1).min().iloc[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates column to indicate movement above and below\n",
    "median movement of the price as the target variable\n",
    "'''\n",
    "\n",
    "conditions = [\n",
    "    np.logical_and(\n",
    "        df_to_analyze['dist_to_max_per_range'] > np.percentile(\n",
    "            df_to_analyze['dist_to_max_per_range'], 50\n",
    "        ),\n",
    "        df_to_analyze['dist_to_min_per_range'] < np.percentile(\n",
    "            df_to_analyze['dist_to_min_per_range'], 50\n",
    "        )\n",
    "    ),\n",
    "    np.logical_and(\n",
    "        df_to_analyze['dist_to_max_per_range'] < np.percentile(\n",
    "            df_to_analyze['dist_to_max_per_range'], 50\n",
    "        ),\n",
    "        df_to_analyze['dist_to_min_per_range'] > np.percentile(\n",
    "            df_to_analyze['dist_to_min_per_range'], 50\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "choices = ['up', 'down']\n",
    "df_to_analyze['y'] = np.select(conditions, choices, default='nothing')\n",
    "#df_to_analyze.y=df_to_analyze.y.shift(-1)\n",
    "df_to_analyze = df_to_analyze.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots distributions of distances\n",
    "to max and to min per range\n",
    "'''\n",
    "\n",
    "#Max\n",
    "plt.subplot(2, 1, 1)\n",
    "ax = df_to_analyze.dist_to_max_per_range.hist(bins=100)\n",
    "ax.axvline(np.quantile(df_to_analyze.dist_to_max_per_range, 0.5))\n",
    "\n",
    "#Min\n",
    "plt.subplot(2, 1, 2)\n",
    "ax = df_to_analyze.dist_to_min_per_range.hist(bins=100)\n",
    "ax.axvline(np.quantile(df_to_analyze.dist_to_max_per_range, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Splits the data into features and targets\n",
    "and further splits it into train and test\n",
    "'''\n",
    "\n",
    "X = df_to_analyze.drop(['dist_to_max_per_range', 'dist_to_min_per_range', 'y'], axis=1)\n",
    "y = df_to_analyze.y\n",
    "\n",
    "#Creates the oldest data as the train set and the newest as the test set\n",
    "train_size = int(df_to_analyze.shape[0] * 0.75)\n",
    "X_train = X.iloc[:train_size, :]\n",
    "y_train = y[:train_size]\n",
    "X_test = X.iloc[train_size:, :]\n",
    "y_test = y.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates the model, fit it,\n",
    "makes predictions\n",
    "'''\n",
    "\n",
    "clf_rf = RandomForestClassifier(\n",
    "    n_estimators = 300 ,\n",
    "    max_depth = 9,\n",
    "    min_samples_split = 3,\n",
    "    min_samples_leaf = 2,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Selects most important features\n",
    "from the previous model, creates new one,\n",
    "fits it, makes predictions\n",
    "'''\n",
    "\n",
    "sel = SelectFromModel(clf_rf)\n",
    "sel.fit(X_train, y_train)\n",
    "\n",
    "X_important_train = sel.transform(X_train)\n",
    "X_important_test = sel.transform(X_test)\n",
    "\n",
    "clf_important = RandomForestClassifier(\n",
    "    n_estimators = 9,\n",
    "    max_depth = 9,\n",
    "    min_samples_split = 3,\n",
    "    min_samples_leaf = 2,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "clf_important.fit(X_important_train, y_train)\n",
    "\n",
    "y_important_pred = clf_important.predict(X_important_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Downloads the model with important features and\n",
    "important features into two separate files\n",
    "'''\n",
    "\n",
    "file_with_model = asset + '_model.sav'\n",
    "pickle.dump(clf_important, open(file_with_model, 'wb'))\n",
    "\n",
    "file_with_features = asset + '_features.sav'\n",
    "pickle.dump(X_train.columns[sel.get_support()], open(file_with_features, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prints classification report\n",
    "for both models\n",
    "'''\n",
    "\n",
    "print('SBER')\n",
    "print('Clf')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Clf_important')\n",
    "print(classification_report(y_test, y_important_pred))\n",
    "print(X_train.columns[sel.get_support()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
