{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from finta import TA\n",
    "from utils.append_indicators import append_indicators\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql://Quotermain:Quotermain233@192.168.0.105:3306/trading_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quotermin/anaconda3/lib/python3.7/site-packages/finta/finta.py:1460: RuntimeWarning: divide by zero encountered in log\n",
      "  (log((1 + _smooth) / (1 - _smooth))).ewm(span=3).mean(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBER\n",
      "Clf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quotermin/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        down       0.45      0.51      0.48      3309\n",
      "     nothing       0.00      0.00      0.00      1832\n",
      "          up       0.42      0.61      0.49      3045\n",
      "\n",
      "   micro avg       0.43      0.43      0.43      8186\n",
      "   macro avg       0.29      0.37      0.33      8186\n",
      "weighted avg       0.34      0.43      0.38      8186\n",
      "\n",
      "Clf_important\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        down       0.43      0.41      0.42      3309\n",
      "     nothing       0.24      0.19      0.22      1832\n",
      "          up       0.39      0.46      0.42      3045\n",
      "\n",
      "   micro avg       0.38      0.38      0.38      8186\n",
      "   macro avg       0.35      0.35      0.35      8186\n",
      "weighted avg       0.37      0.38      0.37      8186\n",
      "\n",
      "Index(['5_open', '5_volume', '6_open', '6_volume', '10_open', '10_high',\n",
      "       '10_volume', '15_volume', '20_volume', '30_volume',\n",
      "       ...\n",
      "       '60_ADL', '60_CHAIKIN', '60_OBV', '60_WOBV', '60_EFI', '60_CFI',\n",
      "       '60_EMV', '60_FISH', '60_VPT', '60_VFI'],\n",
      "      dtype='object', length=225)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assets = [\n",
    "    #'ALRS', \n",
    "    #'CHMF', \n",
    "    #'GAZP', \n",
    "    #'GMKN', \n",
    "    #'HYDR', \n",
    "    #'LKOH', \n",
    "    #'MGNT', \n",
    "    #'MOEX', \n",
    "    #'MTLR', \n",
    "    #'MTSS', \n",
    "    #'NVTK', \n",
    "    #'ROSN', \n",
    "    #'RTKM', \n",
    "    'SBER' \n",
    "    #'SBERP', \n",
    "    #'SIBN', \n",
    "    #'SNGS', \n",
    "    #'SNGSP', \n",
    "    #'TATN', \n",
    "    #'YNDX'\n",
    "]\n",
    "dict_of_tf = {\n",
    "    '1_': 480, #problem\n",
    "    '4_': 120,\n",
    "    '15_': 32,\n",
    "    '30_': 16, #problem\n",
    "    '2_': 240, #problem\n",
    "    '120_': 4,\n",
    "    '20_': 24, #problem\n",
    "    '240_': 2,\n",
    "    '5_': 96,\n",
    "    '6_': 80,\n",
    "    '10_': 48, #problem\n",
    "    '3_': 160,\n",
    "    '60_': 8\n",
    "}\n",
    "\n",
    "list_with_indicators = [\n",
    "            'SMA', 'SMM', 'EMA_13', 'EMA_26', 'EMA_DIF', 'DEMA', 'TEMA', 'TRIMA', 'TRIX',\n",
    "            'VAMA', 'ER', 'ZLEMA', 'WMA', 'HMA', 'EVWMA', 'VWAP', 'SMMA', 'MOM',\n",
    "            'ROC', 'RSI', 'IFT_RSI', 'TR', 'ATR', 'BBWIDTH', 'PERCENT_B', 'ADX', 'STOCH', \n",
    "            'STOCHD', 'STOCHRSI', 'WILLIAMS', 'UO', 'AO', 'TP', 'ADL', 'CHAIKIN', 'MFI',\n",
    "            'OBV', 'WOBV', 'VZO', 'EFI', 'CFI', 'EMV', 'CCI', 'COPP', 'CMO', 'FISH', \n",
    "            'SQZMI', 'VPT', 'FVE', 'VFI', 'MSD', 'return'\n",
    "        ]\n",
    "\n",
    "for asset in assets:\n",
    "    df = pd.read_sql('SELECT * FROM ' + asset + '_train LIMIT 100000', engine)\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')\n",
    "    df = df.dropna()\n",
    "    df = df.set_index('date_time')\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    df['dist_to_max_per_range'] = np.array(df[['close']]\\\n",
    "        .iloc[::-1].rolling(30, min_periods=1).max().iloc[::-1])\\\n",
    "        - np.array(df[['close']])\n",
    "    df['dist_to_min_per_range'] = np.array(df[['close']])\\\n",
    "        - np.array(df[['close']]\\\n",
    "        .iloc[::-1].rolling(30, min_periods=1).min().iloc[::-1])\n",
    "\n",
    "    #Calculates proportion of each row in order book to the apropriate section(bid or offer)\n",
    "    df_offer_count_proportion = df.loc[:, 'offer_count_10':'offer_count_1']\\\n",
    "        .div(df.loc[:, 'offer_count_10':'offer_count_1'].sum(axis=1), axis=0)\n",
    "    df_bid_count_proportion = df.loc[:, 'bid_count_10':'bid_count_1']\\\n",
    "        .div(df.loc[:, 'bid_count_10':'bid_count_1'].sum(axis=1), axis=0)\n",
    "    #Calculates offer/bid ratio per row\n",
    "    offer_bid_ratio = pd.DataFrame(df.loc[:, 'offer_count_10':'offer_count_1'].sum(axis=1) /\\\n",
    "        df.loc[:, 'bid_count_10':'bid_count_1'].sum(axis=1))\n",
    "    df = df.drop([\n",
    "        'offer_count_10', 'offer_count_9', 'offer_count_8', 'offer_count_7',\n",
    "        'offer_count_6', 'offer_count_5', 'offer_count_4', 'offer_count_3',\n",
    "        'offer_count_2', 'offer_count_1', 'bid_count_10', 'bid_count_9', \n",
    "        'bid_count_8', 'bid_count_7',\n",
    "        'bid_count_6', 'bid_count_5', 'bid_count_4', 'bid_count_3',\n",
    "        'bid_count_2', 'bid_count_1'], axis = 1)\n",
    "\n",
    "    #Concatenates single df for analysis\n",
    "    list_of_dfs = [\n",
    "        df,\n",
    "        df_offer_count_proportion, \n",
    "        df_bid_count_proportion, \n",
    "        offer_bid_ratio\n",
    "    ]\n",
    "    df_to_analyze = pd.concat(list_of_dfs, axis=1)\n",
    "\n",
    "    df_to_analyze = df_to_analyze.dropna()\n",
    "\n",
    "    for key in dict_of_tf:\n",
    "        df_to_analyze = append_indicators(\n",
    "            df_to_analyze, key, list_with_indicators\n",
    "        )\n",
    "\n",
    "    df_to_analyze = df_to_analyze.dropna()\n",
    "\n",
    "    #df_to_analyze = df_to_analyze.resample('1T').first()\n",
    "\n",
    "    conditions = [\n",
    "        np.logical_and(\n",
    "            df_to_analyze['dist_to_max_per_range'] > np.percentile(df_to_analyze['dist_to_max_per_range'], 50),\n",
    "            df_to_analyze['dist_to_min_per_range'] < np.percentile(df_to_analyze['dist_to_min_per_range'], 50)\n",
    "        ),\n",
    "        np.logical_and(\n",
    "            df_to_analyze['dist_to_max_per_range'] < np.percentile(df_to_analyze['dist_to_max_per_range'], 50),\n",
    "            df_to_analyze['dist_to_min_per_range'] > np.percentile(df_to_analyze['dist_to_min_per_range'], 50)\n",
    "        )\n",
    "    ]\n",
    "    choices = ['up', 'down']\n",
    "    df_to_analyze['y'] = np.select(conditions, choices, default='nothing')\n",
    "    df_to_analyze.y=df_to_analyze.y.shift(-1) # shifting back because we want to predict using current state\n",
    "    df_to_analyze = df_to_analyze.dropna()\n",
    "\n",
    "    X = df_to_analyze.drop(['dist_to_max_per_range', 'dist_to_min_per_range', 'y'], axis=1)\n",
    "    y = df_to_analyze.y\n",
    "    \n",
    "    train_size = int(df_to_analyze.shape[0] * 0.75)\n",
    "    X_train = X.iloc[:train_size, :]\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X.iloc[train_size:, :]\n",
    "    y_test = y.iloc[train_size:]\n",
    "\n",
    "    clf_rf = RandomForestClassifier(\n",
    "        n_estimators = 300 ,\n",
    "        max_depth = 9,\n",
    "        min_samples_split = 3,\n",
    "        min_samples_leaf = 2,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    clf_rf.fit(X_train, y_train)\n",
    "\n",
    "    sel = SelectFromModel(clf_rf)\n",
    "    sel.fit(X_train, y_train)\n",
    "\n",
    "    X_important_train = sel.transform(X_train)\n",
    "    X_important_test = sel.transform(X_test)\n",
    "\n",
    "    clf_important = RandomForestClassifier(\n",
    "        n_estimators = 9,\n",
    "        max_depth = 9,\n",
    "        min_samples_split = 3,\n",
    "        min_samples_leaf = 2,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    clf_important.fit(X_important_train, y_train)\n",
    "\n",
    "    y_pred = clf_rf.predict(X_test)\n",
    "    y_important_pred = clf_important.predict(X_important_test)\n",
    "    \n",
    "    file_with_model = asset + '_model.sav'\n",
    "    pickle.dump(clf_important, open(file_with_model, 'wb'))\n",
    "    \n",
    "    file_with_features = asset + '_features.sav'\n",
    "    pickle.dump(X_train.columns[sel.get_support()], open(file_with_features, 'wb'))\n",
    "    \n",
    "    print(asset)\n",
    "    print('Clf')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Clf_important')\n",
    "    print(classification_report(y_test, y_important_pred))\n",
    "    print(X_train.columns[sel.get_support()])\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32744, 773)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_analyze.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
